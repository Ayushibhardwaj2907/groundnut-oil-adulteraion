{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda, Dense, Flatten, BatchNormalization,Activation\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1803a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=[224,224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfd370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=r\"C:\\Users\\Nitro\\Desktop\\MajorProject\\Soybean\"\n",
    "valid_path=r\"C:\\Users\\Nitro\\Desktop\\MajorProject\\testdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288a5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962a5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac92da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob(r\"C:\\Users\\Nitro\\Desktop\\MajorProject\\Soybean\\*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f939f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "x = Dense(516, activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x= Dense(256)(x)\n",
    "x = BatchNormalization()(x)  # Batch Normalization\n",
    "x = Activation('relu')(x)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c110dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 516)               12945924  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 516)              2064      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               132352    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,797,594\n",
      "Trainable params: 13,081,362\n",
      "Non-trainable params: 14,716,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "# view the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0058aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "import tensorflow as tf\n",
    "adadelta_optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)\n",
    "model.compile(\n",
    " loss='categorical_crossentropy',\n",
    " optimizer='adam',\n",
    " metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c217fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4416 images belonging to 6 classes.\n",
      "Found 1069 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1/255,validation_split=0.2,\n",
    "                             shear_range = 0.2,\n",
    "                             zoom_range = 0.2,\n",
    "                             horizontal_flip = True)\n",
    "\n",
    "\n",
    "\n",
    "training_set = datagen.flow_from_directory(r\"C:\\Users\\Nitro\\Desktop\\MajorProject\\Soybean\",\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',subset='training')\n",
    "\n",
    "test_set =datagen.flow_from_directory(r\"C:\\Users\\Nitro\\Desktop\\MajorProject\\testdata\",\n",
    "                                        target_size = (224, 224),\n",
    "                                        batch_size = 32,\n",
    "                                        class_mode = 'categorical',subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e658c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro\\AppData\\Local\\Temp\\ipykernel_17196\\2033252562.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "138/138 [==============================] - 553s 4s/step - loss: 0.8929 - accuracy: 0.6544 - val_loss: 0.9139 - val_accuracy: 0.6717\n",
      "Epoch 2/10\n",
      "138/138 [==============================] - 552s 4s/step - loss: 0.6196 - accuracy: 0.7654 - val_loss: 1.7821 - val_accuracy: 0.5201\n",
      "Epoch 3/10\n",
      "138/138 [==============================] - 542s 4s/step - loss: 0.5108 - accuracy: 0.8111 - val_loss: 1.3755 - val_accuracy: 0.6043\n",
      "Epoch 4/10\n",
      "138/138 [==============================] - 539s 4s/step - loss: 0.4417 - accuracy: 0.8361 - val_loss: 0.7364 - val_accuracy: 0.7371\n",
      "Epoch 5/10\n",
      "138/138 [==============================] - 538s 4s/step - loss: 0.4131 - accuracy: 0.8424 - val_loss: 0.9021 - val_accuracy: 0.6866\n",
      "Epoch 6/10\n",
      "138/138 [==============================] - 534s 4s/step - loss: 0.3527 - accuracy: 0.8693 - val_loss: 0.6508 - val_accuracy: 0.7774\n",
      "Epoch 7/10\n",
      "138/138 [==============================] - 540s 4s/step - loss: 0.3466 - accuracy: 0.8664 - val_loss: 0.6694 - val_accuracy: 0.7652\n",
      "Epoch 8/10\n",
      "138/138 [==============================] - 529s 4s/step - loss: 0.3143 - accuracy: 0.8888 - val_loss: 0.5592 - val_accuracy: 0.8064\n",
      "Epoch 9/10\n",
      "138/138 [==============================] - 526s 4s/step - loss: 0.2817 - accuracy: 0.8952 - val_loss: 0.6358 - val_accuracy: 0.8017\n",
      "Epoch 10/10\n",
      "138/138 [==============================] - 858s 6s/step - loss: 0.2689 - accuracy: 0.8986 - val_loss: 0.5885 - val_accuracy: 0.8092\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=10,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ced3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 274s 8s/step\n",
      "Classification Report:\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                    Broken soybeans       0.17      0.15      0.16       198\n",
      "                  Immature soybeans       0.20      0.20      0.20       223\n",
      "                    Intact soybeans       0.19      0.22      0.20       223\n",
      "Part of the original soybean images       0.00      0.00      0.00         1\n",
      "              Skin-damaged soybeans       0.20      0.21      0.20       223\n",
      "                   Spotted soybeans       0.18      0.16      0.17       201\n",
      "\n",
      "                           accuracy                           0.19      1069\n",
      "                          macro avg       0.16      0.16      0.16      1069\n",
      "                       weighted avg       0.19      0.19      0.19      1069\n",
      "\n",
      "Confusion Matrix:\n",
      " [[29 41 46  0 53 29]\n",
      " [40 45 53  0 43 42]\n",
      " [38 44 49  1 49 42]\n",
      " [ 0  0  1  0  0  0]\n",
      " [34 53 56  0 46 34]\n",
      " [32 40 54  0 42 33]]\n"
     ]
    }
   ],
   "source": [
    "# model.save('vgg2clf.h5')\n",
    "# from keras.models import load_model\n",
    "# saved_model=load_model(\"vgg2clf.h5\")\n",
    "# from tensorflow.keras.models import load_model import numpy as np\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# #Make predictions on the validation data\n",
    "\n",
    "# validation_predictions= saved_model.predict(test_set)\n",
    "\n",
    "# # Convert one-hot encoded Labels to class indices\n",
    "\n",
    "# y_true = test_set.classes\n",
    "\n",
    "# y_pred = np.argmax(validation_predictions, axis=1)\n",
    "\n",
    "\n",
    "# # Classification Report\n",
    "\n",
    "# class_report =classification_report(y_true, y_pred, target_names= training_set.class_indices.keys())\n",
    "\n",
    "# print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# #Confusion Matrix confusion_matrix(y_true, y_pred) conf_matrix confusion_mat\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "model.save('vgg2clf.h5')\n",
    "saved_model = load_model(\"vgg2clf.h5\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the validation data\n",
    "validation_predictions = saved_model.predict(test_set)\n",
    "\n",
    "# Convert one-hot encoded Labels to class indices\n",
    "y_true = test_set.classes\n",
    "y_pred = np.argmax(validation_predictions, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred, target_names=training_set.class_indices.keys())\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4263f26f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nitro\\\\Downloads\\\\Photo.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m      3\u001b[0m img\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNitro\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPhoto.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m img1\u001b[38;5;241m=\u001b[39m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m img1_array\u001b[38;5;241m=\u001b[39mimage\u001b[38;5;241m.\u001b[39mimg_to_array(img1)\n\u001b[0;32m      6\u001b[0m img1_array\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mexpand_dims(img1_array,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Nitro\\\\Downloads\\\\Photo.jpg'"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "import keras.utils as image\n",
    "img=(r\"C:\\Users\\Nitro\\Downloads\\Photo.jpg\")\n",
    "img1=image.load_img(img,target_size=(224,224))\n",
    "img1_array=image.img_to_array(img1)\n",
    "img1_array=np.expand_dims(img1_array,axis=0)\n",
    "img1_array/=255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd05d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=saved_model.predict(img1_array)\n",
    "class_indices=training_set.class_indices\n",
    "predicted_class=np.argmax(output,axis=1)\n",
    "predicted_label=[k for k,v in class_indices.items() if v==predicted_class][0]\n",
    "print(\"predicted class\",predicted_label)\n",
    "print(\"predicted class\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx=np.argmax(output)\n",
    "\n",
    "max_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1ba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
